{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1203,"sourceType":"datasetVersion","datasetId":634}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:00.812558Z","iopub.execute_input":"2025-08-18T02:14:00.813491Z","iopub.status.idle":"2025-08-18T02:14:00.834321Z","shell.execute_reply.started":"2025-08-18T02:14:00.813461Z","shell.execute_reply":"2025-08-18T02:14:00.833407Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**10.1- Exploração do banco de dados**\n \n **Assim como foi feito na competição Titanic, é necessário conhecer os banco de dados\n antes que se possa limpar os dados. Nesta sessão será realizada uma exploração a fim de\n entender melhor o banco de dados e identificar possíveis falhas que possam afetar o\n treinamento do modelo.**\n \n **1. O primeiro passo é importar e visualizar o banco de dados, rastrear os valores\n faltantes e identificar os momentos estatísticos existentes:**\n\n**a. Importar o banco de dados utilizando o pandas;**\n\n**b. Identificar o número de valores faltantes em cada colunas;**\n \n**i.Qual sua hipótese para a existência desses valores faltantes?**\n \n**c. Gerar momentos estatísticos do banco de dados;**\n \n**i. Qual coluna parece fora do comum? Por que?**\n \n**d. Utilizando o método “.hist” do pandas realize plots de distribuição dos valores\n das colunas:**\n \n**i. É possível identificar algo de estranho com a coluna “Pressure(millibars)”. Identifique e explique o que há de errado;**\n\n**ii. Utilizando “.loc” do bandas, identifique a quantidade de valores errados.**\n ","metadata":{}},{"cell_type":"code","source":"#letra a:\n\nimport pandas as pd\n\n# a) Importar o banco\ndf = pd.read_csv(\"/kaggle/input/szeged-weather/weatherHistory.csv\")\n\n# Visualizar as 5 primeiras linhas\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:00.835897Z","iopub.execute_input":"2025-08-18T02:14:00.836239Z","iopub.status.idle":"2025-08-18T02:14:01.095138Z","shell.execute_reply.started":"2025-08-18T02:14:00.836209Z","shell.execute_reply":"2025-08-18T02:14:01.094259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#letra b,i\n# b) Contar valores faltantes por coluna\nmissing_values = df.isnull().sum()\nprint(missing_values)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:01.096711Z","iopub.execute_input":"2025-08-18T02:14:01.097712Z","iopub.status.idle":"2025-08-18T02:14:01.123364Z","shell.execute_reply.started":"2025-08-18T02:14:01.097670Z","shell.execute_reply":"2025-08-18T02:14:01.122240Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Como o Dataset é meteorológico os valores costumam falta por falhas temporárias de sensores, perda de conexão com a estação de coleta ou problemas de armazenamento e transmissão dos dados**","metadata":{}},{"cell_type":"code","source":"#letra c,i:\n\n# c) Gerar estatísticas descritivas\nstats = df.describe()\nstats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:01.125319Z","iopub.execute_input":"2025-08-18T02:14:01.125795Z","iopub.status.idle":"2025-08-18T02:14:01.177271Z","shell.execute_reply.started":"2025-08-18T02:14:01.125765Z","shell.execute_reply":"2025-08-18T02:14:01.176407Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Colunas como Pressure ou Temperature(C) podem se destacar se apresentarem valores fora da faixa física esperada, uma coluna fora do comum seria Pressure se apresentar valores muito baixos como  (< 870) ou muito altos como (> 1085)**","metadata":{}},{"cell_type":"code","source":"#letra d,i:\n\nimport matplotlib.pyplot as plt\n\n# d) Plotar histogramas de todas as colunas numéricas\ndf.hist(bins=30, figsize=(15, 10))\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:01.178294Z","iopub.execute_input":"2025-08-18T02:14:01.178551Z","iopub.status.idle":"2025-08-18T02:14:02.700054Z","shell.execute_reply.started":"2025-08-18T02:14:01.178534Z","shell.execute_reply":"2025-08-18T02:14:02.699357Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Observação: se a distribuição mostrar picos artificiais, valores zeros ou negativos, isso sugere erro de medição ou registro incorreto.**","metadata":{}},{"cell_type":"code","source":"#letra d, ii:\n\n# Definir limites plausíveis para pressão atmosférica ao nível do mar\nmin_pressure = 870\nmax_pressure = 1085\n\n# ii) Selecionar linhas com valores fora dessa faixa\npressure_errors = df.loc[(df[\"Pressure (millibars)\"] < min_pressure) | \n                         (df[\"Pressure (millibars)\"] > max_pressure)]\n\n# Exibir quantidade de erros\nlen(pressure_errors)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:02.701042Z","iopub.execute_input":"2025-08-18T02:14:02.701303Z","iopub.status.idle":"2025-08-18T02:14:02.710911Z","shell.execute_reply.started":"2025-08-18T02:14:02.701283Z","shell.execute_reply":"2025-08-18T02:14:02.710084Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Isso indica que 1288 registros da sua base estão com valores de pressão fora da faixa que definimos como fisicamente plausível (870 a 1085 milibares).**\n","metadata":{}},{"cell_type":"markdown","source":"**2. Deve-se agora definir quais são as features categóricas e quais são as numéricas,\n para isso existe o método “.select_dtypes”, aqui será necessário identificar apenas os\n nomes das features.**\n\n**a. Identifique as features numéricas, aquelas que são do tipo “int64” ou “float64”,\n utilizando “.select_dtypes”;**\n\n**b. Identifique as features categóricas, aquelas que são do tipo “object”;**\n\n**i.Uma das features é do tipo object mas não é categórica. Qual é ela?**\n\n**c. Inspecione os valores únicos de cada feature categórica, para garantir que não\n há nenhum valor que ocorra apenas uma vez, o que iria interferir na separação\n do banco de dados em treino e validação (Por que?). Utilize o método\n \".value_counts\" para isso;**\n\n**i.Há uma feature em que existem valores que ocorrem mais de uma vez.\n esse valores serão retirados do banco de dados futuramente;**","metadata":{}},{"cell_type":"code","source":"#letra a:\n\n#- select_dtypes(include=[...]) → filtra colunas pelos tipos especificados.\n#- .columns → retorna apenas o nome das colunas.\n# Features numéricas\nnumeric_features = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\nprint(\"Features numéricas:\", list(numeric_features))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:02.711809Z","iopub.execute_input":"2025-08-18T02:14:02.712140Z","iopub.status.idle":"2025-08-18T02:14:02.730401Z","shell.execute_reply.started":"2025-08-18T02:14:02.712111Z","shell.execute_reply":"2025-08-18T02:14:02.729331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Features categóricas:\n# Features categóricas (tipo object)\ncategorical_features = df.select_dtypes(include=[\"object\"]).columns\nprint(\"Features categóricas:\", list(categorical_features))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:02.731384Z","iopub.execute_input":"2025-08-18T02:14:02.731692Z","iopub.status.idle":"2025-08-18T02:14:02.754373Z","shell.execute_reply.started":"2025-08-18T02:14:02.731660Z","shell.execute_reply":"2025-08-18T02:14:02.753217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#letra b,i:\n\n# Checar colunas object que não são realmente categóricas\nfor col in categorical_features:\n    print(col, type(df[col].iloc[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:02.755452Z","iopub.execute_input":"2025-08-18T02:14:02.755708Z","iopub.status.idle":"2025-08-18T02:14:02.773194Z","shell.execute_reply.started":"2025-08-18T02:14:02.755684Z","shell.execute_reply":"2025-08-18T02:14:02.772367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Converte para datetime garantindo fuso horário UTC\ndf[\"Formatted Date\"] = pd.to_datetime(df[\"Formatted Date\"], utc=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:02.776420Z","iopub.execute_input":"2025-08-18T02:14:02.776693Z","iopub.status.idle":"2025-08-18T02:14:03.987020Z","shell.execute_reply.started":"2025-08-18T02:14:02.776675Z","shell.execute_reply":"2025-08-18T02:14:03.986039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#letra c:\n#Inspecionar valores únicos em cada feature categórica:\nfor col in categorical_features:\n    print(f\"\\nColuna: {col}\")\n    print(df[col].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:03.988315Z","iopub.execute_input":"2025-08-18T02:14:03.988851Z","iopub.status.idle":"2025-08-18T02:14:04.035959Z","shell.execute_reply.started":"2025-08-18T02:14:03.988812Z","shell.execute_reply":"2025-08-18T02:14:04.034874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#letra c,i:\n\n# Exemplo: filtrar valores únicos de uma coluna categórica\nrare_values = df[\"Summary\"].value_counts()[df[\"Summary\"].value_counts() == 1]\nprint(\"Valores raros na coluna Summary:\", rare_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:04.037177Z","iopub.execute_input":"2025-08-18T02:14:04.037526Z","iopub.status.idle":"2025-08-18T02:14:04.055442Z","shell.execute_reply.started":"2025-08-18T02:14:04.037492Z","shell.execute_reply":"2025-08-18T02:14:04.054453Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1. Uma das features do tipo “object” é “Formatted Date” que, na verdade, indica a data\n do registro climático. Essa coluna deverá então ser transformada para o formato de\n data para que seja possível extrair informações úteis para o treinamento do modelo.**\n\n **a. Existe um valor nessa feature que indica o fuso-horário (“+0200”). Para que a\n coluna seja corretamente transformada este valor desse ser retirado;**\n\n**i.Deve ser realizado um processo semelhante ao item 9.2-1, em que\n será utilizado o método “str.split” para retirar o texto “+0200” de todos\n os valores da feature;**\n\n**ii.Use o método \"to_datetime\" para transformar a coluna no formato\n desejado.**\n\n**b. Utilizando os métodos existentes em “.dt” crie novas colunas retirando as\n informações de hora, dia, mês e ano de cada valor de data na feature;**\n \n **i. Utilize os métodos “dt.hour ”, “dt.day ”, “dt.month ”, “dt.year ”;**\n\n **ii. Retire a coluna original de data do banco de dados.**\n\n ","metadata":{}},{"cell_type":"code","source":"# a.i) Remover timezone do final da string (cobre +0200, +0000, +00:00, -0300, Z etc.)\n# - Converte para string para garantir o uso do .str\n# - Usa um único split com regex que pega o sufixo de timezone no final da linha\ndf[\"Formatted Date\"] = (\n    df[\"Formatted Date\"]\n    .astype(str)\n    .str.split(r\"(?:\\s*[+-]\\d{2}:?\\d{2}|Z)$\")  # timezone no fim (com ou sem espaço e com ou sem dois-pontos)\n    .str[0]\n    .str.strip()\n)\n\n# a.ii) Converter para datetime (deixe o pandas inferir o formato)\ndf[\"Formatted Date\"] = pd.to_datetime(df[\"Formatted Date\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:04.056649Z","iopub.execute_input":"2025-08-18T02:14:04.056942Z","iopub.status.idle":"2025-08-18T02:14:04.848317Z","shell.execute_reply.started":"2025-08-18T02:14:04.056923Z","shell.execute_reply":"2025-08-18T02:14:04.847488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# b.i) Criar as novas colunas a partir da data\ndf[\"hour\"] = df[\"Formatted Date\"].dt.hour\ndf[\"day\"] = df[\"Formatted Date\"].dt.day\ndf[\"month\"] = df[\"Formatted Date\"].dt.month\ndf[\"year\"] = df[\"Formatted Date\"].dt.year\n\n# b.ii) Remover a coluna original\ndf.drop(columns=[\"Formatted Date\"], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:04.849249Z","iopub.execute_input":"2025-08-18T02:14:04.849523Z","iopub.status.idle":"2025-08-18T02:14:04.870871Z","shell.execute_reply.started":"2025-08-18T02:14:04.849504Z","shell.execute_reply":"2025-08-18T02:14:04.869900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Checagem rápida:\n\nprint(df[[\"hour\", \"day\", \"month\", \"year\"]].head())\nprint(df[[\"hour\", \"day\", \"month\", \"year\"]].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:04.871943Z","iopub.execute_input":"2025-08-18T02:14:04.872227Z","iopub.status.idle":"2025-08-18T02:14:04.904708Z","shell.execute_reply.started":"2025-08-18T02:14:04.872205Z","shell.execute_reply":"2025-08-18T02:14:04.903901Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. Agora será criada a pipeline para completar a limpeza dos dados, com ela será\n realizada a substituição dos valores faltantes nas colunas “Pressure (millibars)” e\n “Precip Type” e o encoding, utilizando ordinal encoder, das variáveis categóricas. Para\n isso será utilizada a biblioteca Column Transformer do sklearn, mas antes deverão ser\n retirados os dados que não servirão para o treinamento do modelo.**\n \n **a. Retirar a coluna “Loud Cover”, já que contém apenas valores nulos, e linhas\n cujo valor da coluna “Summary” aparecem apenas uma vez;**\n \n **i.Para localizar as linhas que serão retiradas utilize o método “.loc”.**\n \n**b. Definir imputers para variáveis numéricas e para categóricas, e o encoder das\n variáveis categóricas;**\n \n **i.Configurar imputer das variáveis numéricas com simple imputer,\n utilizando a média como substituta do valor faltante e indicando que o\n valor 0 significa valor faltante (valores iguais a 0 em “Pressure\n (millibars)” representam valores faltantes);**\n \n **ii.Criar uma pipeline para tratamento das variáveis categóricas, primeiro\n passo deve ser a substituição dos valores faltantes utilizando simple\n imputer, substituindo o valor faltante por um texto, o segundo passo é\n realizar o encoding utilizando ordinal encoder.**\n \n **c. Criar um objeto do tipo Column Transformer para aplicar os passos criados\n anteriormente.**\n \n **i.Configurar column transformer para fazer o tratamento das variáveis\n numéricas e categóricas.**","metadata":{}},{"cell_type":"code","source":"print(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:04.905654Z","iopub.execute_input":"2025-08-18T02:14:04.906024Z","iopub.status.idle":"2025-08-18T02:14:04.911762Z","shell.execute_reply.started":"2025-08-18T02:14:04.905995Z","shell.execute_reply":"2025-08-18T02:14:04.910566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#A coluna Loud Cloud não existe, pelo que foi visto no código anterior, então façamos o seguinte:\n\n# a) (adaptado) — só remove se existir\nif \"Loud Cover\" in df.columns:\n    df.drop(columns=[\"Loud Cover\"], inplace=True)\n\n# a.i) Remover linhas em que 'Summary' aparece apenas uma vez\nlinhas_unicas = df[\"Summary\"].value_counts() == 1\nvalores_unicos_summary = linhas_unicas[linhas_unicas].index\n\ndf = df.loc[~df[\"Summary\"].isin(valores_unicos_summary)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:04.912597Z","iopub.execute_input":"2025-08-18T02:14:04.913043Z","iopub.status.idle":"2025-08-18T02:14:04.950369Z","shell.execute_reply.started":"2025-08-18T02:14:04.913014Z","shell.execute_reply":"2025-08-18T02:14:04.949588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#b-definir imputer e encoder:\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.pipeline import Pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:04.951514Z","iopub.execute_input":"2025-08-18T02:14:04.951910Z","iopub.status.idle":"2025-08-18T02:14:04.956804Z","shell.execute_reply.started":"2025-08-18T02:14:04.951879Z","shell.execute_reply":"2025-08-18T02:14:04.955670Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#b.i.Pipeline para variávei numéricas:\n\n# Lista de colunas numéricas (refaça a seleção se já não tiver guardada)\nnum_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n\n# Substituir zeros em 'Pressure (millibars)' por NaN\nimport numpy as np\ndf[\"Pressure (millibars)\"] = df[\"Pressure (millibars)\"].replace(0, np.nan)\n\n# Imputer numérico usando média\nnumeric_transformer = SimpleImputer(strategy=\"mean\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:04.957952Z","iopub.execute_input":"2025-08-18T02:14:04.958246Z","iopub.status.idle":"2025-08-18T02:14:04.977781Z","shell.execute_reply.started":"2025-08-18T02:14:04.958221Z","shell.execute_reply":"2025-08-18T02:14:04.976899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Pipeline para variáveis categóricas:\n\n# Lista de colunas categóricas\ncat_cols = df.select_dtypes(include=[\"object\"]).columns\n\n# Pipeline para categóricas: imputação + encoding\ncategorical_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Desconhecido\")),\n    (\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:04.978646Z","iopub.execute_input":"2025-08-18T02:14:04.979224Z","iopub.status.idle":"2025-08-18T02:14:04.993573Z","shell.execute_reply.started":"2025-08-18T02:14:04.979195Z","shell.execute_reply":"2025-08-18T02:14:04.992678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Criar ColumnTransformer:\n\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, num_cols),\n        (\"cat\", categorical_transformer, cat_cols)\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:04.994708Z","iopub.execute_input":"2025-08-18T02:14:04.995059Z","iopub.status.idle":"2025-08-18T02:14:05.012551Z","shell.execute_reply.started":"2025-08-18T02:14:04.995036Z","shell.execute_reply":"2025-08-18T02:14:05.011500Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. O último passo da limpeza dos dados será a separação em treino e validação,\n aplicação da pipeline de limpeza e o encoding da coluna que será o target (“Daily\n summary”).**\n \n**a. Definir quais são as features, o que é o target e separar o banco de dados\n utilizando train-test split, deve-se garantir que todos os valores do target estão\n distribuídos de forma equilibrada na reparação do banco de dados;**\n \n **i.Indicar o target como sendo “Daily Summary” e as features como as\n colunas restantes;**\n \n **ii. Utilizar train-test split, configurado para que o tamanho do banco de\n dados de teste seja 30% do banco de dados original e utilizando o\n target como parâmetro de “stratify”;**\n \n **b. Aplicar pipeline de limpeza do banco de dados;**\n \n **i.Utilize apenas as features dentro da pipeline, ajuste utilizando as\n features de treino e apenas transforme as de teste.**\n \n **c. Utilize label encoder para realizar o encoding do target.**\n \n **i.Da mesma forma, utilize o target de treino para ajustar e apenas\n transforme o de teste.**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# a.i) Definindo target e features\ntarget = \"Daily Summary\"\nX = df.drop(columns=[target])  # todas as colunas exceto o target\ny = df[target]                  # coluna alvo\n\n# a.ii) Dividindo treino e teste com estratificação no target\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.30,         # 30% para teste\n    stratify=y,             # mantém distribuição proporcional das classes\n    random_state=42         # reprodutibilidade\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:05.013495Z","iopub.execute_input":"2025-08-18T02:14:05.013866Z","iopub.status.idle":"2025-08-18T02:14:05.121585Z","shell.execute_reply.started":"2025-08-18T02:14:05.013840Z","shell.execute_reply":"2025-08-18T02:14:05.120440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Exemplo: identifica colunas numéricas e categóricas do X (sem target)\nnum_features = X.select_dtypes(include=['int64', 'float64']).columns\ncat_features = X.select_dtypes(exclude=['int64', 'float64']).columns\n\n# Pipelines para cada tipo\nnumeric_pipeline = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler())\n])\n\ncategorical_pipeline = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n])\n\n# ColumnTransformer só com features — target fica de fora\npreprocessor = ColumnTransformer(transformers=[\n    (\"num\", numeric_pipeline, num_features),\n    (\"cat\", categorical_pipeline, cat_features)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:05.122588Z","iopub.execute_input":"2025-08-18T02:14:05.122833Z","iopub.status.idle":"2025-08-18T02:14:05.134565Z","shell.execute_reply.started":"2025-08-18T02:14:05.122815Z","shell.execute_reply":"2025-08-18T02:14:05.133814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_processed = preprocessor.fit_transform(X_train)\nX_test_processed = preprocessor.transform(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:05.135502Z","iopub.execute_input":"2025-08-18T02:14:05.135789Z","iopub.status.idle":"2025-08-18T02:14:05.454828Z","shell.execute_reply.started":"2025-08-18T02:14:05.135762Z","shell.execute_reply":"2025-08-18T02:14:05.453919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#c-Encoding do target com LabelEncoder\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Criar encoder do target\nlabel_encoder = LabelEncoder()\n\n# Ajustar no treino e transformar\ny_train_encoded = label_encoder.fit_transform(y_train)\n\n# Apenas transformar o teste\ny_test_encoded = label_encoder.transform(y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:05.455617Z","iopub.execute_input":"2025-08-18T02:14:05.455886Z","iopub.status.idle":"2025-08-18T02:14:05.476582Z","shell.execute_reply.started":"2025-08-18T02:14:05.455865Z","shell.execute_reply":"2025-08-18T02:14:05.475554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Cheacegem rápida:\n\nprint(\"Shape treino:\", X_train_processed.shape, \"Target:\", len(y_train_encoded))\nprint(\"Shape teste:\", X_test_processed.shape, \"Target:\", len(y_test_encoded))\nprint(\"Classes do target:\", label_encoder.classes_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:05.477793Z","iopub.execute_input":"2025-08-18T02:14:05.478263Z","iopub.status.idle":"2025-08-18T02:14:05.498551Z","shell.execute_reply.started":"2025-08-18T02:14:05.478225Z","shell.execute_reply":"2025-08-18T02:14:05.497331Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**10.3- Treinamento e Avaliação da rede neural**\n\n**Após realizada a limpeza de dados, o último passo será construir a rede neural,\n treiná-la e avaliá-la. Para isso será utilizado o framework Keras, ao final do treinamento serão\n mostrados gráficos que indiquem a evolução da acurácia e do erro, calculado por cross entropy, em relação ao número de épocas utilizadas para o treinamento. Outro passo\n importante é a indicação para o modelo de que o target é um valor discreto, para isso será\n utilizada uma ferramenta nativa do keras que transforma o target de forma semelhante ao\n one-hot encoding.**\n\n**1. Primeiramente deve-se criar a arquitetura da rede neural, ela terá duas camadas\n escondidas, sendo cada uma com uma saída de dimensão 256, entre elas serão\n utilizados Batch normalization e Dropout. Como funções de ativação serão usados\n “relu” para as camadas escondidas e o “softmax” na camada de saída, esta função\n serve para que o modelo realize predições na forma de classificação não binária, ou\n seja, que seja capaz de prever mais de duas classes.**\n \n **a. Configurar o tamanho da entrada, que é o número de features para\n treinamento, e o tamanho da saída, que será a quantidade de classes do\n target;**\n \n **i.Utilizar o método “.shape” para descobrir quais as dimensões das\n features e “.unique” para descobrir quantas classes existem no target.**\n \n **b. Construir arquitetura do modelo utilizando a descrição já informada;**\n \n**i.Camada de entrada utilizando batch normalization configurando seu\n tamanho a partir das features do banco de dados;**\n \n **ii.Primeira camada escondida utilizando “relu” como função de ativação,\n saída de dimensão 256, batch normalization e dropout de 0.3;**\n \n **iii.Segunda camada escondida igual à primeira;**\n \n **iv.Camada de saída com tamanho igual ao número de classes do target e\n função de ativação “softmax”.**\n \n **c. Realizar tratamento do target utilizando o método “utils.to_categorical” do\n keras.**\n \n **i.Transformar target de treino e de teste utilizando o método do keras;**\n \n **ii.Veja as dimensões da matriz de target e responda, o target é composto\n de quantas classes (Possíveis previsões)?**\n \n **2. Configurar o otimizador, a função de perda que será utilizada e a métrica para\n avaliação do modelo.**\n\n **a. Como otimizador será utilizado “adam”, como função de perda\n “categorical_crossentropy”  e como métrica para avaliação “categorical_accuracy”.**\n \n **3. Por fim, será realizado o treinamento do modelo e plotagem dos resultados das\n avaliações, será utilizado o early stopping para evitar o overfitting. Os hiperparâmetros\n que serão utilizados, como o número de épocas e tamanho de cada batch durante o\n treinamento do modelo podem ser alterados pelo aluno para teste e possível\n aprimoramento.**\n \n**a. Configurar early stopping;\n i.Utilizar a biblioteca do keras para configurar early stopping, com uma\n tolerância de 10 épocas e uma variação mínima no resultado da função\n de perda de 0.001.**\n \n **b. Realizar o treino e armazenar os resultados da avaliação do modelo;**\n \n **i. Utilizar as features e target de treino para treinar o modelo;**\n\n **ii. Utilizar os dados de teste para validar o modelo;**\n\n **iii. Utilizar um tamanho de batch de 3000;**\n\n **iv. Utilizar 100 épocas para treino;**\n\n **v. Indicar utilização de early stopping;**\n\n**c. Para a plotagem deve-se transformar os resultados do treino em um dataframe\n do pandas e plotar os valores de perda e acurácia em função das épocas.**\n \n **i.Utilizar o método “.DataFrame” para transformar resultados em data\n frame do pandas;**\n \n **ii.Utilizar gráfico de linha para plotar os resultados da perda e da acurácia\n emfunção das épocas de treino**\n ","metadata":{}},{"cell_type":"code","source":"#Criar uma arquitetura de rede neural:\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\nimport numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import to_categorical\n\n# a.i) Tamanho da entrada e saída\ninput_dim = X_train_processed.shape[1]      # nº de features\nnum_classes = len(np.unique(y_train_encoded))  # nº de classes no target\n\nprint(f\"Número de features (input_dim): {input_dim}\")\nprint(f\"Número de classes (output_dim): {num_classes}\")\n\n# b) Construção do modelo\nmodel = keras.Sequential()\n\n# b.i) Camada de entrada com Batch Normalization\nmodel.add(layers.Input(shape=(input_dim,)))\nmodel.add(layers.BatchNormalization())\n\n# b.ii) Primeira camada escondida\nmodel.add(layers.Dense(256, activation=\"relu\"))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dropout(0.3))\n\n# b.iii) Segunda camada escondida\nmodel.add(layers.Dense(256, activation=\"relu\"))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dropout(0.3))\n\n# b.iv) Camada de saída com softmax\nmodel.add(layers.Dense(num_classes, activation=\"softmax\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:20:55.459957Z","iopub.execute_input":"2025-08-18T02:20:55.460256Z","iopub.status.idle":"2025-08-18T02:20:55.557411Z","shell.execute_reply.started":"2025-08-18T02:20:55.460235Z","shell.execute_reply":"2025-08-18T02:20:55.556523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#1.c)Tratamento do target(One-Hot Encoding com to_categorical)\n\n\n# i) Transformar target de treino e teste\ny_train_cat = to_categorical(y_train_encoded, num_classes=num_classes)\ny_test_cat = to_categorical(y_test_encoded, num_classes=num_classes)\n\n# ii) Checar dimensões\nprint(\"Formato y_train_cat:\", y_train_cat.shape)\nprint(\"Formato y_test_cat:\", y_test_cat.shape)\nprint(f\"O target possui {y_train_cat.shape[1]} classes possíveis.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:22.786402Z","iopub.execute_input":"2025-08-18T02:14:22.786995Z","iopub.status.idle":"2025-08-18T02:14:23.008455Z","shell.execute_reply.started":"2025-08-18T02:14:22.786964Z","shell.execute_reply":"2025-08-18T02:14:23.007406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#2)Configurar otimizador, função de perda e métrica:\n\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"categorical_crossentropy\",\n    metrics=[\"categorical_accuracy\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:23.009540Z","iopub.execute_input":"2025-08-18T02:14:23.009792Z","iopub.status.idle":"2025-08-18T02:14:23.025937Z","shell.execute_reply.started":"2025-08-18T02:14:23.009771Z","shell.execute_reply":"2025-08-18T02:14:23.024824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#3)Treinamento com Early Stopping:\n\n# a) Configurar EarlyStopping\nearly_stop = keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    patience=10,\n    min_delta=0.001,\n    restore_best_weights=True\n)\n\n# b) Treinar o modelo\nhistory = model.fit(\n    X_train_processed,\n    y_train_cat,\n    validation_data=(X_test_processed, y_test_cat),\n    batch_size=3000,\n    epochs=100,\n    callbacks=[early_stop],\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:14:23.027007Z","iopub.execute_input":"2025-08-18T02:14:23.027356Z","iopub.status.idle":"2025-08-18T02:17:21.261977Z","shell.execute_reply.started":"2025-08-18T02:14:23.027332Z","shell.execute_reply":"2025-08-18T02:17:21.261099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#c)Plotar evolução de perda e acurácia:\n\nimport matplotlib.pyplot as plt\n\n# Transformar histórico em DataFrame\nhist_df = pd.DataFrame(history.history)\n\n# Plotar Perda\nplt.figure(figsize=(12,5))\n\nplt.subplot(1, 2, 1)\nplt.plot(hist_df[\"loss\"], label=\"Treino\")\nplt.plot(hist_df[\"val_loss\"], label=\"Validação\")\nplt.xlabel(\"Épocas\")\nplt.ylabel(\"Perda (Cross-Entropy)\")\nplt.title(\"Evolução da Perda\")\nplt.legend()\n\n# Plotar Acurácia\nplt.subplot(1, 2, 2)\nplt.plot(hist_df[\"categorical_accuracy\"], label=\"Treino\")\nplt.plot(hist_df[\"val_categorical_accuracy\"], label=\"Validação\")\nplt.xlabel(\"Épocas\")\nplt.ylabel(\"Acurácia\")\nplt.title(\"Evolução da Acurácia\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T02:17:21.263629Z","iopub.execute_input":"2025-08-18T02:17:21.263919Z","iopub.status.idle":"2025-08-18T02:17:21.620825Z","shell.execute_reply.started":"2025-08-18T02:17:21.263899Z","shell.execute_reply":"2025-08-18T02:17:21.620003Z"}},"outputs":[],"execution_count":null}]}