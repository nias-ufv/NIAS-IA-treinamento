{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12713174,"sourceType":"datasetVersion","datasetId":8035203}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.194327Z","iopub.execute_input":"2025-08-10T00:28:45.195500Z","iopub.status.idle":"2025-08-10T00:28:45.209055Z","shell.execute_reply.started":"2025-08-10T00:28:45.195460Z","shell.execute_reply":"2025-08-10T00:28:45.208069Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/titaniccap8/train.csv\n/kaggle/input/titaniccap8/test.csv\n/kaggle/input/titaniccap8/gender_submission.csv\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"**Bibliotecas**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nnp.random.seed(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.211005Z","iopub.execute_input":"2025-08-10T00:28:45.211472Z","iopub.status.idle":"2025-08-10T00:28:45.228722Z","shell.execute_reply.started":"2025-08-10T00:28:45.211438Z","shell.execute_reply":"2025-08-10T00:28:45.227742Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"**Reutilizando o notebook do capítulo 7, somente os trechos iniciais:**","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titaniccap8/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/titaniccap8/test.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.229980Z","iopub.execute_input":"2025-08-10T00:28:45.230279Z","iopub.status.idle":"2025-08-10T00:28:45.257499Z","shell.execute_reply.started":"2025-08-10T00:28:45.230259Z","shell.execute_reply":"2025-08-10T00:28:45.256656Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"**1. Será necessário realizar o encoding das variáveis categóricas, no momento, 3\n estratégias que serão utilizadas são o one-hot, label encoding e ordinal encoding.\n Para fazer isso será necessário relembrar quais são as features categóricas, definir a\n estratégia que será utilizada, criar as features codificadas e retirar as categóricas:**","metadata":{}},{"cell_type":"markdown","source":"**a. Utilizar o método “.info( )” para relembrar quais são as features categóricas;**","metadata":{}},{"cell_type":"code","source":"train_data.info( )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.260604Z","iopub.execute_input":"2025-08-10T00:28:45.260990Z","iopub.status.idle":"2025-08-10T00:28:45.273020Z","shell.execute_reply.started":"2025-08-10T00:28:45.260965Z","shell.execute_reply":"2025-08-10T00:28:45.271984Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"test_data.info( )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.274082Z","iopub.execute_input":"2025-08-10T00:28:45.274302Z","iopub.status.idle":"2025-08-10T00:28:45.299079Z","shell.execute_reply.started":"2025-08-10T00:28:45.274285Z","shell.execute_reply":"2025-08-10T00:28:45.298081Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 418 entries, 0 to 417\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  418 non-null    int64  \n 1   Pclass       418 non-null    int64  \n 2   Name         418 non-null    object \n 3   Sex          418 non-null    object \n 4   Age          332 non-null    float64\n 5   SibSp        418 non-null    int64  \n 6   Parch        418 non-null    int64  \n 7   Ticket       418 non-null    object \n 8   Fare         417 non-null    float64\n 9   Cabin        91 non-null     object \n 10  Embarked     418 non-null    object \ndtypes: float64(2), int64(4), object(5)\nmemory usage: 36.1+ KB\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"**b. Utilizar o método “.select_dtypes” para identificar o nome das colunas\n numéricas e categóricas, para numéricas identifique formatos “int64” e\n “float64”, para categóricas identifique “Object”;**","metadata":{}},{"cell_type":"markdown","source":"**Train_data:**","metadata":{}},{"cell_type":"code","source":"# Colunas numéricas\nnum_cols = train_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\nprint(\"Colunas Numéricas:\", num_cols)\n\n# Colunas categóricas\ncat_cols = train_data.select_dtypes(include=['object']).columns.tolist()\nprint(\"Colunas Categóricas:\", cat_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.300020Z","iopub.execute_input":"2025-08-10T00:28:45.300287Z","iopub.status.idle":"2025-08-10T00:28:45.319260Z","shell.execute_reply.started":"2025-08-10T00:28:45.300268Z","shell.execute_reply":"2025-08-10T00:28:45.318437Z"}},"outputs":[{"name":"stdout","text":"Colunas Numéricas: ['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nColunas Categóricas: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"**Test_data:**","metadata":{}},{"cell_type":"code","source":"# Colunas numéricas\nnum_cols = test_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\nprint(\"Colunas Numéricas:\", num_cols)\n\n# Colunas categóricas\ncat_cols = test_data.select_dtypes(include=['object']).columns.tolist()\nprint(\"Colunas Categóricas:\", cat_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.320127Z","iopub.execute_input":"2025-08-10T00:28:45.320404Z","iopub.status.idle":"2025-08-10T00:28:45.339574Z","shell.execute_reply.started":"2025-08-10T00:28:45.320385Z","shell.execute_reply":"2025-08-10T00:28:45.338697Z"}},"outputs":[{"name":"stdout","text":"Colunas Numéricas: ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nColunas Categóricas: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"**c. Temos algumas features categóricas que contém uma ordem clara, ou seja,\n existe o primeiro valor, segundo valor, terceiro valor, assim por diante, já em\n outras, isso não acontece.**\n \n **i.  Para as features que possuem ordem, o label encoder é mais indicado\n como primeira abordagem;**\n \n **ii.Para features sem ordem definida, one-hot encoding pode ser a melhor\n opção;**","metadata":{}},{"cell_type":"markdown","source":"**Letra i:**","metadata":{}},{"cell_type":"markdown","source":"**tain_data:**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ntrain_data['Pclass_encoded'] = le.fit_transform(train_data['Pclass'])\ntrain_data[['Pclass', 'Pclass_encoded']].head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.340890Z","iopub.execute_input":"2025-08-10T00:28:45.341220Z","iopub.status.idle":"2025-08-10T00:28:45.364516Z","shell.execute_reply.started":"2025-08-10T00:28:45.341191Z","shell.execute_reply":"2025-08-10T00:28:45.363601Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"   Pclass  Pclass_encoded\n0       3               2\n1       1               0\n2       3               2\n3       1               0\n4       3               2\n5       3               2\n6       1               0\n7       3               2\n8       3               2\n9       2               1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Pclass_encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"**test_data:**","metadata":{}},{"cell_type":"code","source":"test_data['Pclass_encoded'] = le.fit_transform(test_data['Pclass'])\ntest_data[['Pclass', 'Pclass_encoded']].head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.365488Z","iopub.execute_input":"2025-08-10T00:28:45.365793Z","iopub.status.idle":"2025-08-10T00:28:45.387467Z","shell.execute_reply.started":"2025-08-10T00:28:45.365769Z","shell.execute_reply":"2025-08-10T00:28:45.386421Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"   Pclass  Pclass_encoded\n0       3               2\n1       3               2\n2       2               1\n3       3               2\n4       3               2\n5       3               2\n6       3               2\n7       2               1\n8       3               2\n9       3               2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Pclass_encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"**letra ii:**","metadata":{}},{"cell_type":"markdown","source":"**train_data:**","metadata":{}},{"cell_type":"code","source":"# 1) Deck antes do drop\ntrain_data['Deck'] = train_data['Cabin'].str[0].fillna('U')\ntrain_data.loc[train_data['Deck'] == 'T', 'Deck'] = 'A'\ndeck_order = {'U': 0, 'G': 1, 'F': 2, 'E': 3, 'D': 4, 'C': 5, 'B': 6, 'A': 7}\ntrain_data['Deck_encoded'] = train_data['Deck'].map(deck_order).fillna(0).astype(int)\n\n# Agora pode remover 'Cabin' e 'Deck'\ntrain_data.drop(columns=['Cabin', 'Deck'], inplace=True)\n\n# One-hot apenas nas colunas que ainda existem\n# Sex\nencoder_sex = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\nencoded_sex = encoder_sex.fit_transform(train_data[['Sex']])\nsex_cols = encoder_sex.get_feature_names_out(['Sex'])\ntrain_data = pd.concat(\n    [train_data.drop(columns=['Sex']),\n     pd.DataFrame(encoded_sex, columns=sex_cols, index=train_data.index)],\n    axis=1\n)\n\n# Embarked\nencoder_embarked = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\nencoded_embarked = encoder_embarked.fit_transform(train_data[['Embarked']])\nembarked_cols = encoder_embarked.get_feature_names_out(['Embarked'])\ntrain_data = pd.concat(\n    [train_data.drop(columns=['Embarked']),\n     pd.DataFrame(encoded_embarked, columns=embarked_cols, index=train_data.index)],\n    axis=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.390895Z","iopub.execute_input":"2025-08-10T00:28:45.391179Z","iopub.status.idle":"2025-08-10T00:28:45.422794Z","shell.execute_reply.started":"2025-08-10T00:28:45.391160Z","shell.execute_reply":"2025-08-10T00:28:45.421064Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3064139244.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# One-hot apenas nas colunas que ainda existem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Sex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mencoder_sex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mencoded_sex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_sex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msex_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_sex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'OneHotEncoder' is not defined"],"ename":"NameError","evalue":"name 'OneHotEncoder' is not defined","output_type":"error"}],"execution_count":30},{"cell_type":"markdown","source":"**test_data:**","metadata":{}},{"cell_type":"code","source":"# Deck no test antes do drop\ntest_data['Deck'] = test_data['Cabin'].str[0].fillna('U')\ntest_data.loc[test_data['Deck'] == 'T', 'Deck'] = 'A'\ntest_data['Deck_encoded'] = test_data['Deck'].map(deck_order).fillna(0).astype(int)\ntest_data.drop(columns=['Cabin','Deck'], inplace=True)\n\n# Reutilize encoders treinados no train\nencoded_sex_test = encoder_sex.transform(test_data[['Sex']])\ntest_data = pd.concat(\n    [test_data.drop(columns=['Sex']),\n     pd.DataFrame(encoded_sex_test, columns=sex_cols, index=test_data.index)],\n    axis=1\n)\n\nencoded_embarked_test = encoder_embarked.transform(test_data[['Embarked']])\ntest_data = pd.concat(\n    [test_data.drop(columns=['Embarked']),\n     pd.DataFrame(encoded_embarked_test, columns=embarked_cols, index=test_data.index)],\n    axis=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.423379Z","iopub.status.idle":"2025-08-10T00:28:45.423687Z","shell.execute_reply.started":"2025-08-10T00:28:45.423530Z","shell.execute_reply":"2025-08-10T00:28:45.423541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Garante que test_data tenha as mesmas colunas do train_data\nmissing_cols = set(train_data.columns) - set(test_data.columns)\nfor col in missing_cols:\n    test_data[col] = 0\n\n# Reordena colunas na mesma ordem do train_data\ntest_data = test_data[train_data.columns]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.425209Z","iopub.status.idle":"2025-08-10T00:28:45.425614Z","shell.execute_reply.started":"2025-08-10T00:28:45.425456Z","shell.execute_reply":"2025-08-10T00:28:45.425472Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**d. Criar uma função que realize o one-hot encode e, como saída, retorna um\n novo dataframe com as colunas que resultam da codificação, devidamente\n nomeadas, ao invés das features categóricas:**\n\n  **i.Os argumentos deverão ser: o dataframe que será manipulado e as\n colunas que serão codificadas;**\n\n **ii.Realizar uma cópia do banco de dados que foi dado como argumento;**\n\n **iii.Criar um loop que realize o encoding das colunas presentes no\n argumento da função, as features resultantes devem conter o nome do\n valor que representam (OBS: utilizar o método “.get_feature_names”);**\n\n **iv.Ainda dentro do loop, retirar do dataframe copiado as features\n categóricas e mesclar as features criadas pelo one-hot encoder;** ","metadata":{}},{"cell_type":"markdown","source":"**Para o train_data:**","metadata":{}},{"cell_type":"code","source":"from typing import List\n\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef one_hot_encode_df(_input_df: pd.DataFrame, cols_to_encode: List[str]) -> pd.DataFrame:\n    \"\"\"\n    Codifica colunas categóricas usando OneHotEncoder e retorna o DataFrame com as colunas codificadas substituindo as originais.\n    \"\"\"\n\n    # ii. Cópia do DataFrame\n    train_data = _input_df.copy()\n\n    # Verificação simples para evitar erros\n    missing = [col for col in cols_to_encode if col not in train_data.columns]\n    if missing:\n        raise ValueError(f\"Colunas não encontradas no DataFrame: {missing}\")\n\n    # iii. Loop para codificar colunas\n    for col in cols_to_encode:\n        ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n        encoded = ohe.fit_transform(train_data[[col]])\n\n        # Obtem nomes das novas features codificadas\n        try:\n            new_features = ohe.get_feature_names([col])\n        except AttributeError:\n            new_features = ohe.get_feature_names_out([col])\n\n        # Cria DataFrame das colunas codificadas\n        encoded_df = pd.DataFrame(encoded, columns=new_features, index=train_data.index)\n        encoded_df = encoded_df.astype(int)\n\n        # iv. Remove a coluna original e insere as codificadas\n        train_data.drop(columns=[col], inplace=True)\n        train_data = pd.concat([train_data, encoded_df], axis=1)\n\n    return train_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.427855Z","iopub.status.idle":"2025-08-10T00:28:45.428113Z","shell.execute_reply.started":"2025-08-10T00:28:45.427997Z","shell.execute_reply":"2025-08-10T00:28:45.428007Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Para o test_data:**","metadata":{}},{"cell_type":"code","source":"def one_hot_encode_df(_input_df: pd.DataFrame, cols_to_encode: List[str]) -> pd.DataFrame:\n    \"\"\"\n    Codifica colunas categóricas usando OneHotEncoder e retorna o DataFrame com as colunas codificadas substituindo as originais.\n    \"\"\"\n\n    # ii. Cópia do DataFrame\n    test_data = _input_df.copy()\n\n    # Verificação simples para evitar erros\n    missing = [col for col in cols_to_encode if col not in test_data.columns]\n    if missing:\n        raise ValueError(f\"Colunas não encontradas no DataFrame: {missing}\")\n\n    # iii. Loop para codificar colunas\n    for col in cols_to_encode:\n        ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n        encoded = ohe.fit_transform(test_data[[col]])\n\n        # Obtem nomes das novas features codificadas\n        try:\n            new_features = ohe.get_feature_names([col])\n        except AttributeError:\n            new_features = ohe.get_feature_names_out([col])\n\n        # Cria DataFrame das colunas codificadas\n        encoded_df = pd.DataFrame(encoded, columns=new_features, index=test_data.index)\n        encoded_df = encoded_df.astype(int)\n\n        # iv. Remove a coluna original e insere as codificadas\n        test_data.drop(columns=[col], inplace=True)\n        test_data = pd.concat([test_data, encoded_df], axis=1)\n\n    return test_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.429335Z","iopub.status.idle":"2025-08-10T00:28:45.429718Z","shell.execute_reply.started":"2025-08-10T00:28:45.429503Z","shell.execute_reply":"2025-08-10T00:28:45.429520Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Exemplo de uso do código:**\n\n**Escolha as colunas categóricas que quer utilizar:**\n\ncols_para_codificar = ['Sex', 'Embarked']\n\n**Essas colunas serão transformadas em várias colunas binárias (0 ou 1) representando cada categoria**\n\n**Chame a função que você definiu**\n\ntrain_data_encoded = one_hot_encode_df(train_data, cols_para_codificar)\n\n**Visualize o resultado:**\n\nprint(train_data_encoded.head())\n","metadata":{}},{"cell_type":"markdown","source":"**e. Criar novas features para realizar o label encoding das features que contém\n um ordenamento claro, e depois retirar as features categóricas que foram\n codificadas.**","metadata":{}},{"cell_type":"markdown","source":"**Para o train_data:**","metadata":{}},{"cell_type":"code","source":"# 🔹 1. Extraia o Title a partir de Name\ntrain_data['Title'] = (\n    train_data['Name']\n    .str.extract(r\",\\s*([^\\.]+)\\.\", expand=False)\n    .str.strip()\n)\n\n# 🔹 2. Agrupe títulos raros\ntitle_map = {\n    'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs',\n    'Lady': 'Nobility', 'Countess': 'Nobility', 'Dona': 'Nobility',\n    'Sir': 'Nobility', 'Jonkheer': 'Nobility', 'Don': 'Nobility',\n    'Rev': 'Officer', 'Col': 'Officer', 'Major': 'Officer',\n    'Dr': 'Officer', 'Capt': 'Officer'\n}\ntrain_data['Title'] = train_data['Title'].replace(title_map)\n\n# 🔹 3. Codificação ordinal do Title\ntitle_order = {\n    'Master': 0, 'Miss': 1, 'Mrs': 2,\n    'Mr': 3, 'Officer': 4, 'Nobility': 5\n}\ntrain_data['Title_encoded'] = train_data['Title'].map(title_order).fillna(-1).astype(int)\n\n# 🔹 4. Extraia o Deck a partir de Cabin — se existir!\nif 'Cabin' in train_data.columns:\n    # Evite astype(str) para não transformar NaN em 'nan'\n    train_data['Deck'] = train_data['Cabin'].str[0].fillna('U')\n    train_data.loc[train_data['Deck'] == 'T', 'Deck'] = 'A'\n\n    deck_order = {'U': 0, 'G': 1, 'F': 2, 'E': 3, 'D': 4, 'C': 5, 'B': 6, 'A': 7}\n    train_data['Deck_encoded'] = train_data['Deck'].map(deck_order).fillna(0).astype(int)\nelse:\n    # Se não houver Cabin, ainda garanta a coluna codificada\n    train_data['Deck_encoded'] = 0  # tudo desconhecido\n\n# 🔹 5. Remova colunas temporárias (sem quebrar)\ntrain_data.drop(columns=['Title', 'Deck'], errors='ignore', inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.430932Z","iopub.status.idle":"2025-08-10T00:28:45.431283Z","shell.execute_reply.started":"2025-08-10T00:28:45.431098Z","shell.execute_reply":"2025-08-10T00:28:45.431116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4) Checagem rápida\ntrain_data[['PassengerId', 'Title_encoded', 'Deck_encoded']].head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.432427Z","iopub.status.idle":"2025-08-10T00:28:45.432798Z","shell.execute_reply.started":"2025-08-10T00:28:45.432585Z","shell.execute_reply":"2025-08-10T00:28:45.432602Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Para o test_data:**","metadata":{}},{"cell_type":"code","source":"# 🔹 1. Extraia o Title a partir de Name\ntest_data['Title'] = (\n    test_data['Name']\n    .str.extract(r\",\\s*([^\\.]+)\\.\", expand=False)\n    .str.strip()\n)\n\n# 🔹 2. Agrupe títulos raros\ntitle_map = {\n    'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs',\n    'Lady': 'Nobility', 'Countess': 'Nobility', 'Dona': 'Nobility',\n    'Sir': 'Nobility', 'Jonkheer': 'Nobility', 'Don': 'Nobility',\n    'Rev': 'Officer', 'Col': 'Officer', 'Major': 'Officer',\n    'Dr': 'Officer', 'Capt': 'Officer'\n}\ntest_data['Title'] = test_data['Title'].replace(title_map)\n\n# 🔹 3. Codificação ordinal do Title\ntitle_order = {\n    'Master': 0, 'Miss': 1, 'Mrs': 2,\n    'Mr': 3, 'Officer': 4, 'Nobility': 5\n}\ntest_data['Title_encoded'] = test_data['Title'].map(title_order).fillna(-1).astype(int)\n\n# 🔹 4. Extraia o Deck a partir de Cabin — se existir!\nif 'Cabin' in test_data.columns:\n    test_data['Deck'] = test_data['Cabin'].str[0].fillna('U')\n    test_data.loc[test_data['Deck'] == 'T', 'Deck'] = 'A'\n\n    deck_order = {'U': 0, 'G': 1, 'F': 2, 'E': 3, 'D': 4, 'C': 5, 'B': 6, 'A': 7}\n    test_data['Deck_encoded'] = test_data['Deck'].map(deck_order).fillna(0).astype(int)\nelse:\n    test_data['Deck_encoded'] = 0  # tudo desconhecido\n\n# 🔹 5. Remova colunas temporárias (sem quebrar)\ntest_data.drop(columns=['Title', 'Deck'], errors='ignore', inplace=True)\n\n# 🔹 Checagem rápida\ntest_data[['PassengerId', 'Title_encoded', 'Deck_encoded']].head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.434191Z","iopub.status.idle":"2025-08-10T00:28:45.434494Z","shell.execute_reply.started":"2025-08-10T00:28:45.434373Z","shell.execute_reply":"2025-08-10T00:28:45.434385Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. Agora será realizado um novo treinamento e avaliação de modelo, os passos serão os\n mesmo realizados no item 6.1 g), mas agora com o banco de dados que é resultado\n do encoding das variáveis categóricas. Após essa avaliação, será possível notar a\n evolução da precisão do modelo produzido utilizando essas novas técnicas.**","metadata":{}},{"cell_type":"markdown","source":"**Train_data:**","metadata":{}},{"cell_type":"code","source":"# Imports\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\nimport pandas as pd\n\n# 1) Separe X e y\ndrop_cols = ['Survived', 'Name', 'Ticket']  # Name/Ticket são texto livre\nX = train_data.drop(columns=drop_cols, errors='ignore')\ny = train_data['Survived']\n\n# 2) Garanta que X não tenha colunas de texto restantes\nobj_cols = X.select_dtypes(include=['object']).columns.tolist()\nif obj_cols:\n    print(f\"Removendo colunas categóricas remanescentes de X (não numéricas): {obj_cols}\")\n    X = X.drop(columns=obj_cols)\n\n# 3) Imputação de valores ausentes (mediana para numéricos)\nimputer = SimpleImputer(strategy='median')\nX_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns, index=X.index)\n\n# (opcional) Checagem rápida de NaNs\nassert not X_imputed.isna().any().any(), \"Ainda há NaN após a imputação!\"\n\n# 4) Split\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_imputed, y, test_size=0.25, random_state=0, stratify=y\n)\n\n# 5) Treinar modelo\nmodel = RandomForestClassifier(\n    n_estimators=300,\n    random_state=0,\n    n_jobs=-1\n)\nmodel.fit(X_train, y_train)\n\n# 6) Avaliar\ny_pred = model.predict(X_valid)\nacc = accuracy_score(y_valid, y_pred)\nprint(\"✅ Acurácia com imputação e apenas numéricos:\", round(acc, 4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.435370Z","iopub.status.idle":"2025-08-10T00:28:45.435746Z","shell.execute_reply.started":"2025-08-10T00:28:45.435534Z","shell.execute_reply":"2025-08-10T00:28:45.435549Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**test_data:**","metadata":{}},{"cell_type":"code","source":"# Imports\nimport numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\n\n# 1) Preparar X (numérico) e y a partir do train_data\ndrop_cols = ['Survived', 'Name', 'Ticket']\nX_train_full = train_data.drop(columns=drop_cols, errors='ignore').copy()\ny = train_data['Survived'].copy()\n\n# Remover quaisquer colunas de texto remanescentes (garantir somente numéricos)\nobj_cols_train = X_train_full.select_dtypes(include=['object']).columns.tolist()\nif obj_cols_train:\n    print(f\"Removendo colunas categóricas remanescentes do train: {obj_cols_train}\")\n    X_train_full = X_train_full.drop(columns=obj_cols_train)\n\n# Guardar ordem de colunas do treino para alinhar o test depois\ntrain_cols = X_train_full.columns.tolist()\n\n# 2) Imputação (ajusta no train e reusa no test)\nimputer = SimpleImputer(strategy='median')\nX_train_imputed = pd.DataFrame(\n    imputer.fit_transform(X_train_full),\n    columns=train_cols,\n    index=X_train_full.index\n)\nassert not X_train_imputed.isna().any().any(), \"Ainda há NaN após a imputação no treino!\"\n\n# 3) Treinar modelo (novo objeto, para não depender de 'model' já existente)\nrf_model = RandomForestClassifier(\n    n_estimators=300,\n    random_state=0,\n    n_jobs=-1\n)\nrf_model.fit(X_train_imputed, y)\n\n# 4) Preparar o test_data de forma consistente\nX_test = test_data.drop(columns=['Name', 'Ticket'], errors='ignore').copy()\n\n# Remover quaisquer colunas de texto remanescentes\nobj_cols_test = X_test.select_dtypes(include=['object']).columns.tolist()\nif obj_cols_test:\n    print(f\"Removendo colunas categóricas remanescentes do test: {obj_cols_test}\")\n    X_test = X_test.drop(columns=obj_cols_test)\n\n# 5) Alinhar colunas do test ao conjunto de colunas do train\n#    - adiciona colunas que faltam com 0\n#    - remove colunas extras\nX_test_aligned = X_test.reindex(columns=train_cols, fill_value=0)\n\n# 6) Imputar usando o MESMO imputer treinado no treino (transform, não fit_transform)\nX_test_imputed = pd.DataFrame(\n    imputer.transform(X_test_aligned),\n    columns=train_cols,\n    index=X_test_aligned.index\n)\nassert not X_test_imputed.isna().any().any(), \"Ainda há NaN após a imputação no test!\"\n\n# 7) Prever no test_data\ny_test_pred = rf_model.predict(X_test_imputed)\n\n# 8) Se existir rótulo em test_data, calcular acurácia; caso contrário, mostrar amostra e/ou salvar CSV\nif 'Survived' in test_data.columns:\n    from sklearn.metrics import accuracy_score\n    test_acc = accuracy_score(test_data['Survived'], y_test_pred)\n    print(\"✅ Acurácia no test_data:\", round(test_acc, 4))\nelse:\n    print(\"🔮 Previsões geradas para test_data (primeiros 10):\", y_test_pred[:10])\n    # Exemplo de arquivo de submissão (Kaggle):\n    # submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': y_test_pred})\n    # submission.to_csv('submission.csv', index=False)\n    # print(\"💾 Arquivo 'submission.csv' salvo.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.437189Z","iopub.status.idle":"2025-08-10T00:28:45.437584Z","shell.execute_reply.started":"2025-08-10T00:28:45.437375Z","shell.execute_reply":"2025-08-10T00:28:45.437392Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**8.2- Pipelines**\n\n **1. Deverá ser criada uma função que produz e avalia pipelines que utilizam diversas\n estratégias de imputing de valores nulos e encoding de variáveis categóricas.**\n \n **a. Colocar de volta os valores nulos das colunas “Age” e “Embarked” do\n dataframe de treino, utilizando as colunas do banco de dados original.\n Lembrando que, ao iniciar este notebook, foi feita uma cópia deste banco de\n dados.**","metadata":{}},{"cell_type":"code","source":"def restore_columns_from_original(\n    df_work: pd.DataFrame,\n    df_original_copy: pd.DataFrame,\n    cols_to_restore=(\"Age\", \"Embarked\"),\n    key_col=\"PassengerId\",\n) -> pd.DataFrame:\n    \"\"\"\n    Substitui em df_work as colunas indicadas pelos valores da cópia original,\n    alinhando pela chave key_col. Mantém o restante do df_work inalterado.\n    \"\"\"\n    # Checagens básicas\n    if key_col not in df_work.columns or key_col not in df_original_copy.columns:\n        raise ValueError(f\"Chave '{key_col}' não encontrada em ambos os dataframes.\")\n    for c in cols_to_restore:\n        if c not in df_original_copy.columns:\n            raise ValueError(f\"Coluna '{c}' ausente no df_original_copy.\")\n\n    # Alinha por chave e substitui as colunas\n    restored = df_work.merge(\n        df_original_copy[[key_col, *cols_to_restore]],\n        on=key_col,\n        how=\"left\",\n        suffixes=(\"\", \"__orig\")\n    )\n    # Copia de volta os valores originais para as colunas-alvo\n    for c in cols_to_restore:\n        restored[c] = restored[f\"{c}__orig\"]\n        restored.drop(columns=[f\"{c}__orig\"], inplace=True)\n\n    return restored\n\n# Exemplo de uso (ajuste nomes conforme seu notebook):\n# df_train = restore_columns_from_original(df_train, df_train_original, cols_to_restore=(\"Age\", \"Embarked\"), key_col=\"PassengerId\")\n\n# Verificação rápida:\n# df_train[[\"Age\", \"Embarked\"]].isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.439364Z","iopub.status.idle":"2025-08-10T00:28:45.439662Z","shell.execute_reply.started":"2025-08-10T00:28:45.439531Z","shell.execute_reply":"2025-08-10T00:28:45.439543Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**b. Criar uma função que produza e avalie pipelines que utilizam estratégias\n indicadas nos argumentos:**\n \n **i.Argumentos:**\n \n **● data: banco de dados a partir do qual será produzido o modelo;**\n \n **● encoder: a estratégia de encoding de variáveis categóricas,\n como one-hot ou label encoder;**\n\n **● model: o algoritmo que irá produzir o modelo, inicialmente será\n o random forest classifier;**\n \n **● numerical_imputer: a estratégia usada para substituir valores\n nulos nas features numéricas, como \"mean\" ou \"median\";**\n \n **● categorical_imputer: estratégia para preencher valores nulos\n nas features categóricas, aqui será usada apenas \"mostfrequent\", que é substituir pelo valor que mais aparece no bancode dados;**\n \n **ii. Definir as features e o target que serão usados para treinar o modelo e\n separar o banco de dados utilizando o train-test split.**\n \n **iii. Criar um preprocessor que: para as features numéricas, utilize o\n imputer indicado por “numerical_imputer”; para as categóricas, utilize o\n imputer fornecido em “categorical_imputer” e como encoder utilize o\n argumento “encoder”.**\n \n **iv. Produzir pipeline utilizando o preprocessor previamente construído e o\n modelo do argumento “model”.**\n \n **v. Realizar o treinamento da pipeline, gerar predições e avaliar utilizando\n accuracy score.**\n \n **vi. A função deve retornar o resultado da avaliação em porcentagem.**","metadata":{}},{"cell_type":"code","source":"from typing import Union, Iterable, Optional\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef _make_ohe():\n    # Compatível com versões diferentes do scikit-learn\n    try:\n        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n    except TypeError:\n        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n\ndef train_eval_pipeline(\n    data: pd.DataFrame,\n    encoder: str = \"onehot\",  # \"onehot\" ou \"label\"\n    model=None,               # ex.: RandomForestClassifier(...)\n    numerical_imputer: str = \"median\",      # \"mean\" | \"median\" | \"most_frequent\" | \"constant\"\n    categorical_imputer: str = \"most_frequent\",  # para este exercício, manter \"most_frequent\"\n    target_col: str = \"Survived\",\n    id_cols: Optional[Iterable[str]] = (\"PassengerId\",),\n    test_size: float = 0.2,\n    random_state: int = 42,\n) -> float:\n    \"\"\"\n    Produz e avalia uma pipeline conforme os argumentos e retorna a accuracy em porcentagem.\n    \"\"\"\n    if model is None:\n        model = RandomForestClassifier(n_estimators=300, random_state=random_state, n_jobs=-1)\n\n    if target_col not in data.columns:\n        raise ValueError(f\"target_col '{target_col}' não encontrado em data.\")\n\n    # Define X e y\n    drop_cols = [target_col]\n    if id_cols:\n        drop_cols += [c for c in id_cols if c in data.columns]\n    X = data.drop(columns=drop_cols, errors=\"ignore\")\n    y = data[target_col]\n\n    # Separa colunas numéricas e categóricas\n    numeric_features = X.select_dtypes(include=[\"number\"]).columns.tolist()\n    categorical_features = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n\n    # Split (estratificado quando possível)\n    stratify = y if y.nunique() > 1 else None\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state, stratify=stratify\n    )\n\n    # Imputers\n    num_imputer = SimpleImputer(strategy=numerical_imputer)\n    cat_imputer = SimpleImputer(strategy=categorical_imputer)\n\n    # Encoder\n    if encoder.lower() in (\"onehot\", \"one-hot\", \"ohe\"):\n        cat_encoder = _make_ohe()\n    elif encoder.lower() in (\"label\", \"ordinal\"):\n        cat_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n    else:\n        raise ValueError(\"Parâmetro 'encoder' deve ser 'onehot' ou 'label'.\")\n\n    # Pipelines por tipo\n    from sklearn.pipeline import Pipeline as SkPipeline\n    num_pipe = SkPipeline(steps=[(\"imputer\", num_imputer)])\n    cat_pipe = SkPipeline(steps=[(\"imputer\", cat_imputer), (\"encoder\", cat_encoder)])\n\n    # ColumnTransformer\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"num\", num_pipe, numeric_features),\n            (\"cat\", cat_pipe, categorical_features),\n        ],\n        remainder=\"drop\",\n    )\n\n    # Pipeline final\n    pipe = Pipeline(steps=[(\"preprocess\", preprocessor), (\"model\", model)])\n\n    # Treino\n    pipe.fit(X_train, y_train)\n\n    # Predição e avaliação\n    y_pred = pipe.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    return float(acc * 100.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.440694Z","iopub.status.idle":"2025-08-10T00:28:45.441059Z","shell.execute_reply.started":"2025-08-10T00:28:45.440873Z","shell.execute_reply":"2025-08-10T00:28:45.440889Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Exmplo de uso:**","metadata":{}},{"cell_type":"markdown","source":"**Para o train_data:**","metadata":{}},{"cell_type":"code","source":"# 1) Garantir que os nulos de Age e Embarked foram restaurados\n# df_train = restore_columns_from_original(df_train, df_train_original, (\"Age\", \"Embarked\"), key_col=\"PassengerId\")\n\n# 2) Rodar com One-Hot Encoder e imputação mediana para numéricos\nfrom sklearn.ensemble import RandomForestClassifier\n\nacc_pct_ohe = train_eval_pipeline(\n    data=train_data,\n    encoder=\"onehot\",\n    model=RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1),\n    numerical_imputer=\"median\",\n    categorical_imputer=\"most_frequent\",  # conforme o enunciado\n    target_col=\"Survived\",\n    id_cols=(\"PassengerId\",),\n    test_size=0.2,\n    random_state=42,\n)\nprint(f\"Acurácia (One-Hot): {acc_pct_ohe:.2f}%\")\n\n# 3) Rodar com “label” (OrdinalEncoder) e imputação pela média\nacc_pct_label = train_eval_pipeline(\n    data=train_data,\n    encoder=\"label\",\n    model=RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1),\n    numerical_imputer=\"mean\",\n    categorical_imputer=\"most_frequent\",\n    target_col=\"Survived\",\n    id_cols=(\"PassengerId\",),\n    test_size=0.2,\n    random_state=42,\n)\nprint(f\"Acurácia (Label/Ordinal): {acc_pct_label:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.442708Z","iopub.status.idle":"2025-08-10T00:28:45.443079Z","shell.execute_reply.started":"2025-08-10T00:28:45.442895Z","shell.execute_reply":"2025-08-10T00:28:45.442911Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. Aqui serão feitas duas listas, uma com algumas estratégias de imputing de valores\n nulos e outra com os encoders que foram utilizados até agora. O label encoder deve\n ser substituído pelo ordinal encoder, ambos têm o mesmo resultado, contudo o ordinal\n é feito para decodificar features, e o label, para o target. Acesse estas referências\n para entender sobre a utilização do Simple Imputer e Ordinal encoder**\n\n  **a. Definir as 4 seguintes estratégias para imputing:**\n \n **i.Mean: substitui a o valor nulo pela média**\n \n **ii.Most_frequent: substitui pelo valor mais frequente na feature**\n \n **iii.Median: substitui pela mediana**\n \n **iv.Zero: substitui todos os valores nulos por 0**","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Estratégia 1: Média\nimputer_mean = SimpleImputer(strategy=\"mean\")\n\n# Estratégia 2: Mais frequente\nimputer_most_freq = SimpleImputer(strategy=\"most_frequent\")\n\n# Estratégia 3: Mediana\nimputer_median = SimpleImputer(strategy=\"median\")\n\n# Estratégia 4: Constante (zero)\nimputer_zero = SimpleImputer(strategy=\"constant\", fill_value=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.445947Z","iopub.status.idle":"2025-08-10T00:28:45.446337Z","shell.execute_reply.started":"2025-08-10T00:28:45.446149Z","shell.execute_reply":"2025-08-10T00:28:45.446168Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" **b. Definir as 2 estratégias para encoding:**\n **i.One-Hot encoder**\n **ii.Ordinal encoder**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\n# Estratégia 1: One-Hot Encoder\nonehot_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n\n# Estratégia 2: Ordinal Encoder\nordinal_encoder = OrdinalEncoder()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.447542Z","iopub.status.idle":"2025-08-10T00:28:45.447918Z","shell.execute_reply.started":"2025-08-10T00:28:45.447716Z","shell.execute_reply":"2025-08-10T00:28:45.447732Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**4. A partir da função de criação de pipelines e das listas de encoders e imputers, criar\n um loop que produza todas as combinações possíveis dessas estratégias, e mostre o\n resultado da avaliação de precisão de cada pipeline produzida no loop. (OBS: terá que\n ser usado um “for” dentro de outro).**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\n\n# 🧠 Defina alvo e separa X e y\ntarget_col = \"Survived\"\nX = train_data.drop(columns=[target_col], errors=\"ignore\")\ny = train_data[target_col]\n\n# 🔍 Detecta colunas numéricas e categóricas\nnum_cols = X.select_dtypes(include=[\"number\"]).columns.tolist()\ncat_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n\n# ⚙️ Estratégias de imputação numérica\nnum_imputers = [\n    (\"mean\", SimpleImputer(strategy=\"mean\")),\n    (\"median\", SimpleImputer(strategy=\"median\")),\n    (\"most_frequent\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"zero\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n]\n\n# 🧩 Estratégias de codificação\nencoders = [\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"if_binary\", sparse_output=False)),\n    (\"ordinal\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n]\n\n\n# ⚖️ Validação cruzada estratificada\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# 📊 Armazenar resultados\nresults = []\n\n# 🔁 Loop de todas as combinações\nfor imp_name, imp in num_imputers:\n    num_pipe = Pipeline([\n        (\"imputer\", imp),\n        (\"scaler\", StandardScaler()),\n    ])\n\n    for enc_name, enc in encoders:\n        cat_pipe = Pipeline([\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"encoder\", enc),\n        ])\n\n        preprocessor = ColumnTransformer(transformers=[\n            (\"num\", num_pipe, num_cols),\n            (\"cat\", cat_pipe, cat_cols),\n        ])\n\n        model = LogisticRegression(\n            solver=\"lbfgs\",\n            max_iter=5000,   # <- aumenta iterações\n            C=0.5,           # <- mais regularização (experimente 0.1 também)\n            random_state=42\n        )\n\n        pipeline = Pipeline([\n            (\"prep\", preprocessor),\n            (\"model\", model),\n        ])\n\n        scores = cross_val_score(pipeline, X, y, cv=cv, scoring=\"accuracy\")\n        results.append({\n            \"Imputação Numérica\": imp_name,\n            \"Codificador\": enc_name,\n            \"Acurácia Média\": round(scores.mean() * 100, 2),\n            \"Desvio Padrão\": round(scores.std() * 100, 2)\n        })\n\n\n# 📋 Exibir resultados em ordem\nresults_df = pd.DataFrame(results).sort_values(by=\"Acurácia Média\", ascending=False)\nprint(\"🏆 Ranking de Precisão por Estratégia:\")\nprint(results_df.reset_index(drop=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.450096Z","iopub.status.idle":"2025-08-10T00:28:45.450417Z","shell.execute_reply.started":"2025-08-10T00:28:45.450275Z","shell.execute_reply":"2025-08-10T00:28:45.450289Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**5. Avalie se alguma dessas estratégias usadas separadamente, gerou uma precisão do\n modelo maior que suas utilizações mescladas, que foi a estratégia realizada para\n produzir o modelo antes das pipelines.**","metadata":{}},{"cell_type":"code","source":"#Modelo simples (baseline), sem pipeline \n# ⚙️ Imputação numérica\nX_num = X[num_cols].fillna(0)  # imputação zero\nX_num = StandardScaler().fit_transform(X_num)\n\n# 🧩 Codificação categórica\nX_cat = X[cat_cols].fillna(\"missing\")\nencoder = OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\", sparse_output=False)\nX_cat_encoded = encoder.fit_transform(X_cat)\n\n# 🔗 Junta tudo\nX_final = np.hstack([X_num, X_cat_encoded])\n\n# 🎯 RegLog sem pipeline\nmodel = LogisticRegression(max_iter=5000, C=0.5, random_state=42)\nscores = cross_val_score(model, X_final, y, cv=cv, scoring=\"accuracy\")\n\nprint(\"⚖️ Acurácia (modelo simples):\", round(scores.mean() * 100, 2), \"+/-\", round(scores.std() * 100, 2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:45.451634Z","iopub.status.idle":"2025-08-10T00:28:45.452027Z","shell.execute_reply.started":"2025-08-10T00:28:45.451840Z","shell.execute_reply":"2025-08-10T00:28:45.451856Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Como o modelo simples teve a mesma acurácia da Pipeline isso levanta a questão se vale a pena manter o modelo com a pipeline. A resposta é sim, pois ele possui vantagens que o outro não tem, vatagens essas que são as seguintes:**\n\n**Organização: deixa o código replicável e lipo**\n**Reprodutibilidade: evita esquecimentos em etapas de pré-processamento.**\n**Validação correta: garante que transformações sejam feitas dentro dos folds, prevenindo data leakage.**\n**Escalabilidade: se você mudar para outro modelo ou aplicar grid search, é só plugar no pipeline.**\n\n","metadata":{}},{"cell_type":"markdown","source":"****","metadata":{}},{"cell_type":"markdown","source":"**8.3- Cross Validation e Gradient Boosting**\n\n**1. O cross validation é uma maneira melhor para avaliar os modelos com os quais\n estamos lidando. Portanto, o modelo que melhor se saiu nos teste até agora, deverá\n ser avaliado utilizando este método. Como resultado será utilizada a média entre as\n avaliações, lembre-se de utilizar o scoring = “accuracy”.**","metadata":{}},{"cell_type":"markdown","source":"O modelo que apresentou melhor desempenho foi o de imputação numérica zero, codificador: one hot,acuracia média: 82.16%, desvio padrão: 1.55.","metadata":{}},{"cell_type":"markdown","source":"**2. Assim como o Random Forest, o XGBoost também tem seu equivalente como\n classifier. O Gradient Boosting Classifier será utilizado agora para gerar um novo\n modelo, a partir do dataframe que foi manipulado com as melhores estratégias até\n agora.**\n\n **a. Importar o Gradient Boosting Classifier, utilizando random state = 0 e\n n_iter_no_change = 100 (Entender o que são esses parâmetros a partir da\n documentação do algoritmo);**\n\n **b. Determinar as features e o target que serão utilizados para gerar o modelo;**\n\n **c. Treinar e avaliar o modelo utilizando cross validation;**\n\n **d. A partir da nota gerada pela avaliação, determinar qual algoritmo se saiu\n melhor, Random Forest ou Gradient Boosting.**","metadata":{}},{"cell_type":"markdown","source":"**Importar o modelo e entender os parâmetros:**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:53.947959Z","iopub.execute_input":"2025-08-10T00:28:53.948242Z","iopub.status.idle":"2025-08-10T00:28:53.953019Z","shell.execute_reply.started":"2025-08-10T00:28:53.948224Z","shell.execute_reply":"2025-08-10T00:28:53.952048Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"**Definir features e target**","metadata":{}},{"cell_type":"code","source":"# Definir variáveis\ntarget_col = \"Survived\"\ndrop_cols = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"]  # você pode ajustar conforme seu dataset\n\n# Separar X e y\nX = train_data.drop(columns=[target_col] + drop_cols, errors=\"ignore\").copy()\ny = train_data[target_col].copy()\n\n# Codificar variáveis categóricas com OneHotEncoder\nobj_cols = X.select_dtypes(include=\"object\").columns.tolist()\n\nif obj_cols:\n    print(\"🔍 Categóricas encontradas:\", obj_cols)\n    encoder = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    encoded = encoder.fit_transform(X[obj_cols])\n    encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(obj_cols), index=X.index)\n    X = pd.concat([X.drop(columns=obj_cols), encoded_df], axis=1)\n\n# Imputar nulos\nX = pd.DataFrame(SimpleImputer(strategy=\"median\").fit_transform(X), columns=X.columns, index=X.index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:28:56.053482Z","iopub.execute_input":"2025-08-10T00:28:56.053861Z","iopub.status.idle":"2025-08-10T00:28:56.082796Z","shell.execute_reply.started":"2025-08-10T00:28:56.053837Z","shell.execute_reply":"2025-08-10T00:28:56.081574Z"}},"outputs":[{"name":"stdout","text":"🔍 Categóricas encontradas: ['Sex', 'Embarked']\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"**Treinar e avaliar com cross-validation:**","metadata":{}},{"cell_type":"code","source":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscoring = \"roc_auc\"\n\ngb = GradientBoostingClassifier(\n    random_state=0,\n    n_iter_no_change=100,\n    validation_fraction=0.15\n)\n\nrf = RandomForestClassifier(\n    n_estimators=500,\n    max_depth=None,\n    min_samples_leaf=1,\n    random_state=0,\n    n_jobs=-1\n)\n\nscores_gb = cross_val_score(gb, X, y, cv=cv, scoring=scoring, n_jobs=-1)\nscores_rf = cross_val_score(rf, X, y, cv=cv, scoring=scoring, n_jobs=-1)\n\nprint(f\"🌲 Random Forest AUC => média: {scores_rf.mean():.4f} ± {scores_rf.std():.4f}\")\nprint(f\"🎯 Gradient Boosting AUC => média: {scores_gb.mean():.4f} ± {scores_gb.std():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:29:02.332185Z","iopub.execute_input":"2025-08-10T00:29:02.332545Z","iopub.status.idle":"2025-08-10T00:29:08.017374Z","shell.execute_reply.started":"2025-08-10T00:29:02.332513Z","shell.execute_reply":"2025-08-10T00:29:08.016441Z"}},"outputs":[{"name":"stdout","text":"🌲 Random Forest AUC => média: 0.8690 ± 0.0264\n🎯 Gradient Boosting AUC => média: 0.8616 ± 0.0366\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"**Comparar com RandoForest e dizer qual foi melhor:**","metadata":{}},{"cell_type":"code","source":"# Escolher o melhor modelo\nmelhor_est = gb if scores_gb.mean() >= scores_rf.mean() else rf\nmelhor_nome = \"Gradient Boosting\" if scores_gb.mean() >= scores_rf.mean() else \"Random Forest\"\nprint(f\"💡 Modelo vencedor: {melhor_nome}\")\nmelhor_est.fit(X, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:29:11.630636Z","iopub.execute_input":"2025-08-10T00:29:11.631335Z","iopub.status.idle":"2025-08-10T00:29:13.307625Z","shell.execute_reply.started":"2025-08-10T00:29:11.631301Z","shell.execute_reply":"2025-08-10T00:29:13.306740Z"}},"outputs":[{"name":"stdout","text":"💡 Modelo vencedor: Random Forest\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=0)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=0)</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":34},{"cell_type":"markdown","source":"**Para o test_data:**","metadata":{}},{"cell_type":"code","source":"# Preparar test_data (replicar mesmo tratamento de X)\nX_test = test_data.drop(columns=drop_cols, errors=\"ignore\").copy()\n\n# Codificar categóricas\nobj_test_cols = X_test.select_dtypes(include=\"object\").columns.tolist()\n\nif obj_test_cols:\n    encoded_test = encoder.transform(X_test[obj_test_cols])\n    encoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(obj_test_cols), index=X_test.index)\n    X_test = pd.concat([X_test.drop(columns=obj_test_cols), encoded_test_df], axis=1)\n\n# Imputar nulos\nX_test = pd.DataFrame(SimpleImputer(strategy=\"median\").fit_transform(X_test), columns=X_test.columns, index=X_test.index)\n\n# Alinhar colunas com treino\nX_test = X_test.reindex(columns=X.columns, fill_value=0)\n\n# Previsão\npreds_test = melhor_est.predict_proba(X_test)[:, 1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T00:29:21.772365Z","iopub.execute_input":"2025-08-10T00:29:21.772763Z","iopub.status.idle":"2025-08-10T00:29:21.987685Z","shell.execute_reply.started":"2025-08-10T00:29:21.772737Z","shell.execute_reply":"2025-08-10T00:29:21.986493Z"}},"outputs":[],"execution_count":36}]}